<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<link rel="stylesheet" type="text/css" href="../../css/style.css" />
<meta http-equiv="content-type" content="text/html; charset=US-ASCII" />
<meta name="Robots" content="index,follow" />
<meta name="author" content="bert" />
<meta name="keywords" content="bayesian, machine learning, probability theory, evidence-based, kalman filtering, EM algorithm, information processing, signal processing, decision theory" /><title>5MB20 Adaptive Information Processing Course Web Page</title>

</head>

<body>

<div id="head">
<div id="title">5MB20</div>
<div id="menu">

<ul>
<li> <a href="../../index.html" title="home">Home</a>
</li>
<li> <a href="../../research/research.html" title="Research">Research</a> </li>
<li class="active"> <a href="#" title="Teaching">Teaching</a> </li>
<li> <a href="../../vitae/CV-Bert-de-Vries.html" title="Resume">Resum&#233;</a> </li>
</ul>

</div><!--- id="menu" --> 
</div><!--- id="head" -->

<div id="body_wrapper">
<div id="body">
<div id="all">
<div class="top"></div>
<div class="content">

<h1 style="text-align: center;">5MB20 Adaptive Information
Processing</h1>

<h2 style="text-align: center;">2014/15 Course Web Page</h2>

<!--
<p style="color: red; font-size: 1.2em;">NOTE: ROOM CHANGE ON MONDAYS, SEE <a href="#schedule">COURSE SCHEDULE TABLE</a> BELOW</p>
-->

<h1>Course Description</h1>
Signal processing is primarily concerned with filtering, smoothing
and
prediction of time-ordered sequences. Information processing extends
this application terrain to such (seemingly) varied areas as pattern
classification, language processing, bio-informatics, error-correcting
coding and database searching, just to name a few. In this course,
using fundamental concepts of probability and information theory, we
present an introduction to the design of such information processing
systems. This course, which can also be taken as an <span style="font-weight: bold;">Introduction to Machine Learning</span>,
is structured in two parts: <br />

<h3>Part 1: Linear Gaussian Models
and the EM Algorithm</h3>
First, we present the fundamentals of machine learning from a
(Bayesian) probability theory perspective. A classic machine learning
task is
to determine good estimates for the parameters of a given model
structure from a set of observed data. We introduce Maximum
Likelihood (ML) estimation as an effective method to estimate model
parameters. It
turns out that for an important class of models, the Linear Gaussian
Models, ML estimation problems can be solved using the
Expectation-Maximization (EM) algorithm. We derive ML estimation
methods and discover the connections for many Linear Gaussian Models,
including Gaussian mixture models, Kalman filters, hidden
Markov models, principal and independent component analysis circuits
and neural
networks.

<h3>Part 2: Model Complexity
Control and the MDL Principle</h3>
If we assume more than one possible model then we can find a good
estimate for the parameters for each class. However, we still
have to
select a good class. In part 2, the notion of 'Stochastic Complexity'
will be developed and the Minimum Description Length (MDL) principle
will be
used to select an appropriate model.<br />
<br />

<h1>When</h1>

In the 2014/15 academic year, this class is taught in the 2nd quarter, starting on 10-Nov-2014. We meet in Mondays and Wednesdays, 
at alternating frequencies of 4 hours and 2 hours per week. 
Please check the <a href="https://venus.tue.nl/owinfo-cgi/owi_0695.opl?language=NL&vakcode=5MB20">TUE information site</a> for more detailed information on meeting times and location. 

<!--<ul>
<li> Mondays (13:45 - 15:30, 3rd and 4th hour) in room PT 1.05
</li>
<li> Fridays (13:30 - 15:15, 5th and 6th hour) in room <a href="http://w3.tue.nl/nl/de_universiteit/route_en_plattegrond/plattegrond/">PT 1.05</a></li>
</ul> -->

<h1>Instructors</h1>
<a href="http://www.sps.ele.tue.nl/members/T.J.Tjalkens/">Dr.ir.
Tjalling J. Tjalkens</a> and <a href="http://www.sps.ele.tue.nl/members/B.Vries/">prof.dr.ir.
Bert de Vries</a>. Send us an email or drop by if you want more
information about the class.

<h1>Prerequisites</h1>
Mathematical maturity equivalent to undergraduate engineering program.
Some matlab programming skills is helpful.

<h1>Material</h1>
<span style="float: right;"><img src="./i/Bishop-book-cover.jpg" alt="book" width="100" /> </span>We will use the following text
book:
<p></p>
<div style="color: blue; font-size: 1.1em;">Christopher M.
Bishop, <a href="http://research.microsoft.com/en-us/um/people/cmbishop/PRML/index.htm"><em>Pattern
Recognition
and Machine Learning</em></a>. Springer, 2006.
</div>

<p>
Try to get the book before classes start.
Next to the reading assignments in the book, further material consists 
of lecture notes
(slides) and exercises, which will be made available through this
website. You're strongly advised to download the slides from this
website and take them with you to the class in order to add your
personal comments. </p>

<p>
In the 2014/15 academic year, there will be written exams on <span style="color: red;">22-January 2015</span> and <span style="color: red;">(2nd date TBD)</span>  (see the <a href="http://owinfo.tue.nl">official
TU/e announcement site</a>). <span style="color: red;">You cannot bring notes or books to the exam. Needed formulas are supplied at the exam sheet.</span> To get some extra practice, here are
some exercises

</p>
<ul>

<li> <!--to be posted -->
<a href="./devriesb/exercises/2010-5MB20-exercises-part-1.pdf">Exercises
for part 1</a> here. And the same <a href="./devriesb/exercises/2010-5MB20-exercises-part-1-with-solutions.pdf">exercises
for part-1 with solutions</a>.
</li>
<li><a href="./TjalkensT/exercises.pdf">Exercises for part 2</a> here. <a href="./TjalkensT/hints.pdf">Some hints</a> are available too. </li>
</ul>

<p>
Furthermore, we have some recent old exams here. This is an excellent
preparation for the exam:
</p>
<ul>

<li> <a href="./exams/100624/100624-5mb20-exam.pdf">exam
of June 24, 2010</a> and also <a href="./exams/100624/100624-5mb20-exam-with-solutions.pdf">with
solutions</a>.
</li>

<li> <a href="./exams/110414/110414-5mb20-exam.pdf">exam
of April 14, 2011</a> and also <a href="./exams/110414/110414-5mb20-exam-with-solutions.pdf">with
solutions</a>.
</li>

<li> <a href="./exams/110620/110620-5mb20-exam.pdf">exam
of June 20, 2011</a> and also <a href="./exams/110620/110620-5mb20-exam-with-solutions.pdf">with
solutions</a>.
</li>

</ul>

<h1>Further references</h1>

<ul>
<li>The following 'cheat sheets' by Sam
Roweis are handy when doing the exercises.

<ul>
<li><a href="./devriesb/papers/RoweisS-gaussian_identities.pdf">Gaussian
Identities</a></li>
<li><a href="./devriesb/papers/RoweisS-matrix_identities.pdf">Matrix
Identities</a></li>
</ul>
</li>
</ul>


<h1>Video</h1>

The 2007 class meetings were recorded and can be <a href="http://videocollege.tue.nl">viewed</a>
if you have a valid TU/e account. Note however that the 2014 class will
change a bit relative to the 2007 class. Talk to us before you plan to
follow the class only from video.
<hr style="height: 3px;" />

<h1><a name="schedule">Course Schedule</a></h1>

<h2>Part 1: Linear
Gaussian
Models
and the EM Algorithm</h2>
<b>Instructor</b>:
<a href="http://www.sps.ele.tue.nl/members/B.Vries/">Prof.dr.ir.
Bert de Vries</a>
<br /><br />

<table border="1" cols="3" width="98%">
  <tbody>
    <tr>
      <td align="center" width="20%"><b>Date / Location</b></td>
      <td align="center" width="35%"><b>Topics</b></td>
      <td align="center"><b>Materials</b></td>
    </tr>
      
<!-- first --> <tr>
      <td>
      <div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Mon Nov-10</span><br />
15:45 - 17:30 <br />
IPO-0.98</div>
      </td>
      <td> (0) Administrative issues<br />
(1) Introduction <br />
(2) Prob. theory review <br />
(3) Bayesian Machine Learning</td>
      <td>
         
      <a href="./devriesb/slides/5MB20-part-1-slides-all.pdf">ALL SLIDES</a>;      

<br /><br />
Bishop pp. (1) 1-4 , (2) 12-20, (3) 21-24 <br />
      <br />
      <span style="color: rgb(0, 100, 0);">optional reading:</span><br />
      <a href="http://research.microsoft.com/en-us/um/people/minka/papers/nuances.html">Minka2006
- Nuances of prob. theory</a><br />
      <a href="./devriesb/papers/BruyninkxH02-Bayesian-probability.pdf">Bruyninkx2002
- Bayesian probability</a><br />
      </td>
    </tr>
      
<!-- 2nd --> <tr>
      <td>
      <div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Wed Nov-12</span><br />
10:45 - 12:30 <br />
AUD-5</div>
      </td>
      <td>
(4) Working with Gaussians <br />
(5) Density estimation <br />
(6) Linear Regression</td>
      <td>
<!--
<a href="./devriesb/slides/5MB20-lect-2-article.pdf">slides</a><br /> <br />
<span style="color: rgb(0, 100, 0);">background
reading:</span><br />
-->Bishop
(4) 85-93, (5) 67-70, 74-76, 93-94, (6) 140-144<br />
      <br />
      <span style="color: rgb(0, 100, 0);">matlab demo:</span>
      <a href="./devriesb/matlab/demo_classification.m.txt">demo_classification.m</a>
      </td>
    </tr>
      
      <!-- 3rd --> <tr>
      <td>
       <div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Mon Nov-17</span><br />
15:45 - 17:30 <br />
IPO-0.98</div>
      </td>
      <td>
7.1) Generative classification <br />
(7.2) Discriminative class. <br />
(8) Gaussian mixture models
      </td>
      <td>
<!--
<a href="./devriesb/slides/5MB20-lect-3-article.pdf">slides</a><br /> <br />
<span style="color: rgb(0, 100, 0);">background
reading:</span><br />
-->Bishop
(7.1) 196-202, (7.2)
203-206, (8) 430-439<br />
      <br />
      <span style="color: rgb(0, 100, 0);">optional reading:</span><br />
      <a href="./devriesb/papers/MinkaT05-discriminative-models-not-discriminative-training.pdf">Minka2005-Discriminative
Models</a><br />
      <span style="color: rgb(0, 100, 0);">matlab demo:</span>
      <a href="./devriesb/matlab/demo_gmm.m.txt">demo_gmm.m</a>, <a href="./devriesb/matlab/circle.m.txt">circle.m</a>
      </td>
    </tr>

<!-- 4th--> <tr>
      <td>
       <div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Mon Nov-24</span><br />
15:45 - 17:30 <br />
IPO-0.98</div>
      </td>
      <td>
(9) EM algorithm<br />
(10.1) Factor Analysis and PCA <br />
(10.2) Independent Component Analysis
      </td>
      <td>
<!--
<a href="./devriesb/slides/5MB20-lect-4-article.pdf">slides</a><br /> 
<span style="color: rgb(0, 100, 0);">background
reading:</span><br />
-->Bishop
(9) 55-57, 439-443, 450-455, (10.1) 570-573, 577-580, 584-586, (10.2)
591-592<br />
      <br />
      <span style="color: rgb(0, 100, 0);">optional reading:</span><br />
      <a href="./devriesb/papers/SinghA05-the-EM-algorithm.pdf">Singh2005 - EM
Algorithm</a><br />
      </td>
    </tr>
      
<!-- 5th --> <tr>
      <td>
   <div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Wed Nov-26</span><br />
10:45 - 12:30 <br />
AUD-5</div>
      </td>
      <td> (11.1) Hidden Markov Models<br />
(11.2) Kalman Filters </td>
      <td><!--
<a href="./devriesb/slides/2010-5MB20-lect-5-article.pdf">slides</a><br /><br />
<span style="color: rgb(0, 100, 0);">background
reading:</span>
-->
Bishop (11.1) 605-615, (11.2) 635-641<br />
      <br />
      <span style="color: rgb(0, 100, 0);">optional reading:</span><br />
      <a href="./devriesb/papers/MinkaT99-from-HMM-to-LDS.pdf">Minka1999
- From HMM to LDS</a><br />
      </td>
    </tr>
      
<!-- 6th -->
    <tr>
      <td>
             <div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Mon Dec-1</span><br />
15:45 - 17:30 <br />
IPO-0.98</div>
      </td>
      <td> Review </td>
      <td> <span style="color: rgb(0, 100, 0);">optional:</span><br />
      <a href="./devriesb/papers/RoweisS99-Unifying_Review_of_Linear_Gaussian_Models.pdf">Roweis:
 Unifying Review</a><br />
       <a href="http://goo.gl/jhml0">Ghahramani
- Bayesian Modelling (VIDEO)</a><br />     
      </td>
    </tr>
		  
  </tbody>
</table>


<!-- ************************************ *** FROM HERE ON TJALLING'S PART *** ************************************ -->
<hr style="height: 3px;" />
<h2>Part 2: Model Complexity Control and the MDL Principle </h2>

<b>Instructor</b>:
<a href="http://www.sps.ele.tue.nl/members/T.J.Tjalkens/">Dr.ir.
Tjalling J. Tjalkens</a>
<br /><br />

<table border="1" cols="3" width="98%">
<tbody>

<tr>
<td width="20%" align="center"><b>Date / Location</b></td>
<td width="35%" align="center"><b>Topics</b></td>
<td align="center"><b>Materials</b></td>
</tr>

<!-- 7th class --> 
<tr>
<td><div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Mon Dec-8</span><br />
15:45 - 17:30 <br />
IPO-0.98</div>
</td>

  <td>Part A: The Bayesian Information Criterion</td>
  <td colspan="1" rowspan="7">

      
 <a href="TjalkensT/alltext4.pdf">Printable version</a> of the slides.<br />

      <a href="TjalkensT/alltext.pdf">The slides</a> as shown during the lectures. <br />

      Background reading in Bishop is listed in the slides.

      <br />
      A <a href="TjalkensT/summary-AIP.pdf">summary</a> and explanation
      of Markov structures is also available.<br />
      
</td>
</tr>


<!-- 8th class --> 
<tr>
<td>   <div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Wed Dec-10</span><br />
10:45 - 12:30 <br />
AUD-5</div>
</td>
  <td>Part A: The Bayesian Information Criterion (continued)</td>
<!--  <td>

See above for the slides of Part A.</td>
-->
</tr>
  
  
<!-- 9th class--> 
<tr>
<td><div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Mon Dec-15</span><br />
15:45 - 17:30 <br />
IPO-0.98</div>
</td>

  <td>Part B: Bayesian model estimation and the Context-tree model selection.</td>
<!--  <td>

Part B text.<br />
      <a href="TjalkensT/part2b-4.pdf">Printable version</a> of the slides.<br />
      <a href="TjalkensT/part2b.pdf">The slides</a> as shown during the lectures.

  </td>
-->
</tr>

<!-- break --> 
      <tr>
      <td colspan="2">
      <div style="color: rgb(255, 0, 0);" align="center">
          Holiday break (22, 24 and 29 Dec) </div>
      </td>
    </tr>


<!-- 10th class--> 
<tr>
<td><div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Mon Jan-5</span><br />
15:45 - 17:30 <br />
IPO-0.98</div>
</td>

  <td>Part B: Bayesian model estimation and the Context-tree model selection (continued).</td>
<!--  <td>

See above for the slides of Part B.

</td>
-->
</tr>

<!-- 11th class--> 
<tr>
<td><div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Wed Jan-7</span><br />
10:45 - 12:30 <br />
AUD-5</div></td>
  <td>Part C: Descriptive complexity <br />
      Wrap-up.
  </td>
 <!-- <td>

Part C text.<br />
      <a href="TjalkensT/part2c-4.pdf">Printable version</a> of the slides.<br />
      <a href="TjalkensT/part2c.pdf">The slides</a> as shown during the lectures.

</td>
-->
</tr>

<!-- 12th class--> 
    
<tr>
<td><div align="center"><span style="color: rgb(0, 0, 110); font-weight: bold;">Mon Jan-12</span><br />
15:45 - 17:30 <br />
IPO-0.98</div>
</td>

  <td>Part C: Descriptive complexity (continued)<br />
      Wrap-up.
  </td>
 
</tr> 
   

</tbody>
</table>


<!-- 
Additional reading:<br />
T.M. Cover and J.A. Thomas, <span style="font-style: italic;">Elements
of Information Theory</span>, Wiley. Chapters 13 and 14.<br />
Rissanen: <a href="http://www.mdl-research.org/pub/lectures.pdf">J.
Lectures on statistical modeling theory</a>. Chapter
5.<br />&nbsp;
-->


          </div><!-- class="content" -->
          <div class="bottom"></div>
        </div> <!-- id="all" -->

        <div class="clearer"></div>
      </div> <!-- id="body" -->
      <div class="clearer"></div>

    </div><!-- id="body_wrapper" -->
    <div id="end_body"></div>
  </body>
</html>

