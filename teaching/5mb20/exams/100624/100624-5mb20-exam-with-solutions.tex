\documentclass[a4paper]{article}
\usepackage{a4wide}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb}
\usepackage{tikz}
\usetikzlibrary{arrows,trees,automata}
\usepackage{aip}%             't beste als aip de laatste in de lijst is.

\input{5MB20-preamble.tex}
   

\newcommand{\tjboxed}[1]{\par\begin{center}\tikz \node[draw,text width=13cm,inner sep=3pt,line width=1pt] {\parbox{13cm}{#1}};\end{center}}




%                        HEADER INFORMATIE
\examendatum{24 June 2010}                    % Datum van het examen.
\examentijd{09u00--12u00}                       % Tijd van het examen.

\begin{document}

\begin{exam}
%
% algemene vorm van invoer:
% gebruik het 'environment'
%       \begin{vraag}{xxx}
%         ...
%       \end{vraag}
% Hierin wordt in xxx beschreven hoe de verschillende onderdelen
% scoren. Per vraag zijn 10 punten te verdelen. Bij het nakijken per
% onderdeel een geheel aantal punten tussen 0 en aangegeven aantal
% toekennen.
%
% Deelvragen worden aangegeven door het environment
% \begin{deelvraag}
%   ...
% \end{deelvraag}
%

% Hier is vraag 1
\begin{vraag}{each sub-question 2 points. Total 10 points}
For each of the following sub-questions, you are asked to provide a \emph{short but essential} answer. You should not need more than five sentences per answer.

\begin{deelvraag}
    Explain shortly how Bayes rule relates to machine learning. In your answer, you may assume a model $\mathcal{M}$ with prior distribution $p(\mathcal{M})$ and an observed data set $D$.
\tjboxed{
$$ \underbrace{p(\mathcal{M}|D)}_{\text{posterior}} = \frac{p(D|\mathcal{M})}{p(D)}\underbrace{p(\mathcal{M})}_{\text{prior}}
$$
Bayes rule relates what we know about a model before (prior) and after (posterior) having seen the data. The difference between the prior and posterior distributions for the model can be interpreted as a `machine learning' effect. (Alternative answers are also possible).
}%end tjboxed
\end{deelvraag}

\begin{deelvraag}
    Explain the relation between Bayesian estimation, Maximum a Posteriori (MAP) estimation and Maximum Likelihood (ML) estimation. You may assume a context of a given model structure with unknown parameters $\theta$ and an observed data set $D$.
\tjboxed{
\begin{eqnarray*}
\hat \theta_{\text{bayes}} = \int_\theta \theta p(\theta|D) \d{\theta} & \text{(Bayes est.)}\\
\hat \theta_{\text{map}} = \arg\max_\theta p(\theta|D) = \arg\max_\theta p(D|\theta)p(\theta) & \text{(MAP)}\\
\hat \theta_{\text{ml}} = \arg\max_\theta p(D|\theta) & \text{(ML)}
\end{eqnarray*}
Bayes estimation picks the mean from the posterior $p(\theta|D)$. MAP picks the mode from $p(\theta|D)$. ML is MAP with uniform prior. (Alternative answers are also possible).
}%end tjboxed
\end{deelvraag}

  The following two sub-questions relate to a (Factor Analysis) model $x_n=\Lambda z_n + v_n$ for an observed data set $D=\{x_1,\dots,x_N\}$. The modeling assumptions include $z_n \sim \mathcal{N}(0,I)$, $v_n \sim \mathcal{N}(0,\Psi)$ and $\varepsilon[z_nv_n^T]=0$.

\begin{deelvraag}
Show that the covariance matrix of the observed data $x_n$ is equal to $\Lambda\Lambda^T + \Psi$.
\tjboxed{
\begin{align*}
\epsilon[x] &= \epsilon[\Lambda z + v] = \Lambda \epsilon[z] + \epsilon[v] = 0\\
\var[x] &= \epsilon[(x-\epsilon[x])(x-\epsilon[x])^T] =  \epsilon[(\Lambda z +v)(\Lambda z +v)^T] \\
    &= \Lambda \epsilon[zz^T] \Lambda^T + \epsilon[vv^T] = \Lambda \Lambda^T + \Psi
\end{align*}
}%end tjboxed    
\end{deelvraag}

\begin{deelvraag} Why is this model not interesting for unconstrained $\Psi$? How does probabilistic PCA handle this problem?
\tjboxed{
(a) Because setting $\Lambda=0$ would result in a `regular' gaussian model with covariance matrix $\Psi$; i.o.w. it's no more interesting than any other gaussian model.\\
(b) If $\Psi$ is diagonal, then all correlations between the components in $x$ \emph{must} be absorbed ('explained') by the rank-$K$ matrix $\Lambda \Lambda^T$. In pPCA, this is achieved by the constraint $\Psi=\sigma^2 I$.
}%end tjboxed    
\end{deelvraag}

\begin{deelvraag}
Which of the following statements are justified? You can pick more than one and read the sign `$\sim$' as: `is similar to'. (Just pick the correct statements; no explanation needed).\\
1: discriminative classification $\sim$ density estimation\\
2: generative classification $\sim$ density estimation\\
3: hidden Markov model $\sim$ factor analysis through time\\
4: Kalman filtering $\sim$ unsupervised regression through time\\
5: clustering $\sim$ supervised classification
\tjboxed{
2 and 4 are correct.
}%end tjboxed    
\end{deelvraag}


%\begin{deelvraag}
%What's the significance of the forward ($\alpha-$) recursion in Linear Dynamical Systems?
%\end{deelvraag}
%
%\begin{deelvraag}
%
%In a particular model with hidden variables, the log-likelihood can be worked out to the following expression:
%$$
%\ell(\theta;D) = \sum_n \log \left(\sum_k \N(x_n|\mu_k,\Sigma_k)\pi_k\right)
%$$
%Is there a problem when trying to do maximum likelihood estimation? Explain shortly.
%\end{deelvraag}
%
%\begin{deelvraag}
%The maximum likelihood estimate (MLE) of the class-conditional mean in a classification problem can be expressed as
%$$
%\hat\mu_k = \frac{\sum_n y_n^k x_n}{\sum_n y_n^k}
%$$
%and the M-step update for the cluster mean in a clustering problem is given by
%$$
%\hat\mu_k = \frac{\sum_n r_n^k x_n}{\sum_n r_n^k}
%$$
%Explain the relation between $y_n^k$ and $r_n^k$.
%\end{deelvraag}

\end{vraag}

% Hier is vraag 2
\begin{vraag}{a) 2 points; b) 2 points; c) 2 points; d) 1 point; e) 1 point; f) 2 points. Total 10 points}


(EM for 2-component Gaussian mixture). Consider an observed IID data set $D=\{x_1,\ldots,x_N\}$ and a proposed model,
\begin{align*}
p(x_n) &= \sum_{k=0}^1 p(x_n,z_n=k|\pi)\\
    &=  p(z_n=1|\pi)p(x_n|z_n=1) + p(z_n=0|\pi)p(x_n|z_n=0)\\
    &= \pi\N_1(x_n) +(1-\pi)\N_0(x_n)
\end{align*}


where we used shorthand notation $\N_k(x_n)\equiv (2\pi\sigma_k^2)^{-1/2}\exp\left(-(x_n-\mu_k)^2/(2\sigma_k^2)\right)$ for the Gaussian distribution. We assume that the parameters $\theta=(\mu_0,\sigma_0^2,\mu_1,\sigma_1^2)$ are known, but the \emph{mixing proportion} parameter $\pi$ is unknown.  The random variable $z_n \in \{0,1\}$ is an \emph{unobserved} `cluster label'.  In this question we will derive an EM-algorithm for maximum likelihood estimation of $\pi$. Let's assume that a estimate $\hat \pi = \pi^{(j)}$ is available from the previous iteration. We will now focus on the $(j+1)$-th iteration in the EM algorithm.

\begin{deelvraag}
Describe shortly the E- and M-steps in the $(j+1)$-th iteration of the EM-algorithm. In particular, complete the following equation set (fill in the stars) for the $(j+1)$-th iteration and shortly describe the meaning of the equations: (Note: the expression $\langle f(x)\rangle_{p(x)}$ stands for the expectation of $f(x)$ w.r.t. probability distribution $p(x)$.)
\begin{align*}
q_n^{(j+1)} &= p(\star|\star) \quad\text{(E-step)}\\
\pi^{(j+1)} &= \arg\max_\pi \langle\star\rangle_{\star} \quad \text{(M-step)}
\end{align*}
\tjboxed{
\begin{align*}
q_n^{(j+1)} &= p(z_n|x_n,\pi^{(j)}) \quad\text{(E-step)}\\
\pi^{(j+1)} &= \arg\max_\pi \langle \sum_n p(x_n,z_n|\pi)\rangle_{q_n^{(j+1)}} \quad \text{(M-step)}
\end{align*}
\textbf{E-step}: $q_n^{(t+1)}$ is the posterior probability of $z_n$, given observation $x_n$ and an estimate $\pi^{(j)}$ from the previous iteration. $q_n$ represents our knowledge about $z_n$. \\
\textbf{M-step}: Maximizes the expected complete-data log-likelihood. Through Jensen's inequality it can be proved that this procedure increases the (observed data) log-likelihood $p(D|\pi)$.
}%end tjboxed    
\end{deelvraag}

%
%\begin{deelvraag}
%Compute the log-likelihood $\ell(\pi)=\log p(D|\pi)$. Why
%\tjboxed{
%\begin{align*}
%\ell(\pi)&=\log p(D|\pi)=\sum_n \log [\pi\N_1(x_n) +(1-\pi)\N_0(x_n)]
%\end{align*}
%}%end tjboxed    
%\end{deelvraag}

\begin{deelvraag}
Work out $p(x_n,z_n=1|\pi)$ (hint: use product rule).  Work out $p(x_n,z_n=0|\pi)$. And now work out the joint distribution $p(x_n,z_n|\pi)$ to a Bernoulli distribution (as in eq.A1, see formula cheat sheet). In this question, you need to work out the probabilities in terms of $z_n$, $\N_0(x_n)$, $\N_1(x_n)$ and $\pi$.
\tjboxed{
$p(x_n,z_n=1) = p(x_n|z_n=1)p(z_n=1)=\pi\N_1(x_n)$\\
$p(x_n,z_n=0) = p(x_n|z_n=0)p(z_n=0)=(1-\pi)\N_0(x_n)$\\
$p(x_n,z_n|\pi) = [\pi\N_1(x_n)]^{z_n}[(1-\pi)\N_0(x_n)]^{1-z_n}$
}%end tjboxed    
\end{deelvraag}

\begin{deelvraag}
Show that the complete-data log-likelihood $\ell_c(\pi) = \sum_n \log p(x_n,z_n|\pi)$ can be worked out to
\begin{equation}
\ell_c(\pi) = \sum_n z_n \log \frac{\pi\N_1(x_n)}{(1-\pi)\N_0(x_n)} + \sum_n \log(1-\pi)\N_0(x_n)
\label{eq:complete-data-log-likelihood}
\end{equation}
\tjboxed{
\begin{align*}
\ell_c(\pi) &= \sum_n \log p(x_n,z_n|\pi)\\
    &=\sum_n \log  \left([\pi\N_1(x_n)]^{z_n}[(1-\pi)\N_0(x_n)]^{1-z_n}\right) \notag\\
    &=\sum_n z_n \log\pi\N_1(x_n) + \sum_n(1-z_n) \log(1-\pi)\N_0(x_n) \notag\\
    &=\sum_n z_n \log \frac{\pi\N_1(x_n)}{(1-\pi)\N_0(x_n)} + \sum_n \log(1-\pi)\N_0(x_n)
\end{align*}
}%end tjboxed    
\end{deelvraag}

To finalize the E-step, we now take the expectation of the complete-data log-likelihood with respect to the posterior distribution $p(z_n|x_n,\pi^{(j)})$. It follows from Eq.1 that we need to compute the expected value of $z_n$.  We'll compute the expected value of $z_n$ in two stages:

%\begin{deelvraag}
%Why do we maximize in the M-step the \emph{expected} complete-data log-likelihood rather than the `regular' complete-data log-likelihood?
%\tjboxed{
%The `regular' complete-data log-likelihood is a function of the unobserved RV $z_n$ and hence cannot be evaluated. The \emph{expected} complete-data log-likelihood can be evaluated (and hence) its maximum can be searched.\\
%Alternative answer: It can be proven through Jensen's inequality that maximizing the \emph{expected} complete-data log-likelihood also maximizes the observed-data log-likelihood.
%}%end tjboxed    
%\end{deelvraag}


\begin{deelvraag}
First show that the expectation $\sum_{\{z_n\}} z_n \cdot p(z_n|x_n,\pi^{(j)})$ can be worked out as follows:
$$
\sum_{\{z_n\}} z_n p(z_n|x_n,\pi^{(j)}) = p(z_n=1|x_n,\pi^{(j)})
$$
\tjboxed{
\begin{align*} \sum_{\{z_n\}}z_np(z_n|x_n,\pi) &= 0\cdot p(z_n=0|x_n,\pi)+1\cdot p(z_n=1|x_n,\pi)\\
    &=p(z_n=1|x_n,\pi)
\end{align*}
}%end tjboxed    
\end{deelvraag}

\begin{deelvraag}
And now use Bayes rule to work out an expression for $p(z_n=1|x_n,\pi^{(j)})$ in terms of $\pi^{(j)}$, $\N_0(x_n)$ and $\N_1(x_n)$.
\tjboxed{
\begin{align*}
p(z_n=1|x_n,\pi^{(j)}) &= \frac{p(x_n|z_n=1)p(z_n=1|\pi^{(j)})}{\sum_k p(x_n|z_n=k)p(z_n=k|\pi^{(j)})}\\
    &=\frac{\pi^{(j)} \N_1(x_n)}{\pi^{(j)} \N_1(x_n) + (1-\pi^{(j)})\N_0(x_n)}
\end{align*}
}%end tjboxed    
\end{deelvraag}

If we use shorthand notation $\zeta_n = p(z_n=1|x_n,\pi^{(j)})$, then the expected complete-data log-likelihood can be written as
\begin{align*}
\langle\ell_c(\pi)\rangle &=\sum_n \zeta_n\log \frac{\pi\N_1(x_n)}{(1-\pi)\N_0(x_n)} + \sum_n \log(1-\pi)\N_0(x_n)
\end{align*}

\begin{deelvraag}
Set $\partial \langle\ell_c(\pi)\rangle / \partial \pi=0$ and obtain an expression for $\pi^{(j+1)}$ in terms of $\pi^{(j)}$, $\N_0(x_n)$ and $\N_1(x_n)$ (i.e. write down the $(j+1)$-th iteration of the M-step).
\tjboxed{
\begin{align*}
\frac{\partial \langle\ell_c(\pi)\rangle}{  \partial \pi} &= \sum_n \frac{\zeta_n}{\pi} + \sum_n \frac{\zeta_n}{1-\pi} - \sum_n \frac{1}{1-\pi}\\
&= \frac{1}{\pi(1-\pi)}\sum_n\left( \zeta_n - n\pi\right)
\end{align*}
Set derivative to zero and it follows that
\begin{align*}
\pi^{(j+1)} &= \frac{1}{N}\sum_n \zeta_n\\
    &= \frac{\pi^{(j)} \N_1(x_n)}{\pi^{(j)} \N_1(x_n) + (1-\pi^{(j)})\N_0(x_n)}
\end{align*}
}%end tjboxed    
\end{deelvraag}
\end{vraag}


%%%%%%%%%%%%%%%%
%%%
%%% MDL vragen
%%%

%%%
%%% Vraag Kolmogorov
%%%
\begin{vraag}{a) 3 points; b) 3 points. Total 6 points}
You observe some data $x^n$. You ask two experts to explain the data.

Expert $A$ uses a data compression system that needs 1537 bits to describe the parameters
of the model and 438 bits to describe the data given the model.

Expert $B$ gives you a system that needs 1325 bits for the parameters and 650 bits for
the data, given the model.
\begin{deelvraag}
  Which expert's result do you prefer?\\
  Explain (briefly) why you select that experts result.
  \tjboxed{%
    The total description length of $A$'s result is $1537+438=1975$ bits.
    For expert $B$ the total description length is $1325+650=1975$ bits. So both experts
    achieve the same complexity. In accordance with Occam's razor I prefer expert
    $B$'s explanation because his/her model is less complex.
  }
\end{deelvraag}
\begin{deelvraag}
  You ask two additional experts.

  Expert $C$ gives you a model with a parameter description length of 1471 bits
  and a data description that needs 450 bits.

  Expert $D$ gives you a model with a parameter description length of 1464 bits
  and a data description that needs 543 bits.

  Of the four experts $A$ to $D$, which result do you prefer, and why?
  \tjboxed{%
    The total complexity for expert $C$ is $1471+450=1921$ bits and for
    expert $D$ it is $1464+543=2007$ bits. Expert $D$ explanation is more
    complex than any of the three others
    so I reject it in accordance with the MDL principle. For the same
    reason I prefer expert $C$'s explanation, because it has the smallest overall
    complexity although the model complexity is larger than for expert $B$.
  }
\end{deelvraag}
\end{vraag}



%%%
%%% Vraag 4 Laplace
%%%
\newcommand{\ddx}{\frac{\partial}{\partial x}}
\newcommand{\ddxsq}{\frac{\partial^2}{\partial x^2}}
\begin{vraag}{a) 3 points; b) 3 points. Total 6 points}
    Let $X$ be a real valued random variable with probability
    density
    \[ p_X(x) = \frac{e^{-x^2/2}}{\sqrt{2\pi}},\quad\text{for all $x$}. \]
    Also $Y$ is a real valued random variable with conditional
    density
    \[ p_{Y|X}(y|x) = \frac{e^{-(y-x)^2/2}}{\sqrt{2\pi}},\quad\text{for
    all $x$ and $y$}. \]
    \begin{deelvraag}
        Give an (integral) expression for $p_Y(y)$.\\
        Do not try to evaluate the integral.
        \tjboxed{%
            \[ p_Y(y) = \int_{-\infty}^{\infty} p_X(x)p_{Y|X}(y|x)\,dx =
                \int_{-\infty}^{\infty}
                \frac{e^{-\frac12(x^2+(y-x)^2)}}{2\pi}\,dx \]
        }
    \end{deelvraag}
    \begin{deelvraag}
        Approximate $p_Y(y)$ using the Laplace approximation.\\
        Give the detailed derivation, not just the answer.\\
        Hint: You may use the following results.
        \begin{align*}
          \intertext{Let}
          g(x) &= \frac{e^{-x^2/2}}{\sqrt{2\pi}},\quad\text{and} \\
          h(x) &= \frac{e^{-(y-x)^2/2}}{\sqrt{2\pi}},\quad\text{for some real value $y$.} \\
          \intertext{Then}
          \ddx g(x) &= -xg(x) \\
          \ddxsq g(x) &= (x^2-1)g(x) \\
          \ddx h(x) &= (y-x)h(x) \\
          \ddxsq h(x) &= ((y-x)^2-1)h(x) \\
        \end{align*}
        \tjboxed{%
        Using the hint we determine the first derivative of
        \begin{align*}
        	f(x) &= g(x)h(x), \\
        	\ddx f(x) &= \ddx g(x)\cdot h(x) = -xg(x)h(x)+g(x)(y-x)h(x) = (y-2x)f(x)\\
        	\intertext{Setting this to zero gives}
        	y-2x&= 0; \quad \text{so}\quad x=\frac12y. \\
        	\ddx \ln f(x) &= \frac{\ddx f(x)}{f(x)} = (y-2x) \\
        	\ddxsq \ln f(x) &= \ddx (y-2x) = -2. \\
        	\intertext{So, we find $A=2$, see lecture notes, and thus}
        	p_Y(y) &= \int_{-\infty}^{\infty}f(x)\,dx\approx f(\frac{y}{2})\sqrt{\frac{2\pi}{A}} \\
        	       &= g(\frac{y}{2})h(\frac{y}{2})\sqrt{\frac{2\pi}{A}} \\
        	       &= \frac{1}{\sqrt{2\pi\cdot2}}e^{-y^2/4}.
        \end{align*}
        So $Y$ is a Gaussian with mean $m=0$ and variance $\sigma^2=2$.
        }
    \end{deelvraag}
\end{vraag}
%%%
%%% Einde vraag 4: Laplace
%%%

%%% Vraag 5: MDL
%%%

\begin{vraag}{a) 1 point; b) 1 point; c) 2 points; d) 2 points; e) 2 points. Total 8 points}
We implement an e-mail spam filter using two features that we can extract from an e-mail.
A feature can be the occurrence of a particular word or phrase in the e-mail.

Given an e-mail $E$ we denote the extracted features by $F$ and $G$.\\
$F=1$ means that feature $F$ is present in the e-mail $E$.\\
$F=0$ means that feature $F$ is absent. And likewise for feature $G$.\\
The variable $C$ indicates whether $E$ is spam ($C=1$) or not ($C=0$).

We are given 247 e-mails that are already classified. The following table shows how many e-mails contained certain features and the classification.
\begin{center}
  \begin{tabular}{rrr|r}
  $F$ & $G$ & $C$ & nr of e-mails \\
  \hline
  0 & 0 & 0 & 15 \\
  0 & 0 & 1 & 28 \\
  0 & 1 & 0 & 18 \\
  0 & 1 & 1 & 25 \\
  1 & 0 & 0 &  8 \\
  1 & 0 & 1 & 75 \\
  1 & 1 & 0 & 10 \\
  1 & 1 & 1 & 68
  \end{tabular}
\end{center}
\begin{deelvraag}
From the table given above you can determine probability estimates using the maximum likelihood estimates. e.g.\ the probability $P(C=1)$, i.e.\ the probability that an email will be spam, is approximated by:
\[ P(C=1) = \frac{\text{\# of e-mails with }C=1}{\text{total \# of e-mails}} =
   \frac{196}{247}=0.7935.
\]
Note that the method using a beta prior would be better suited but we'll use the maximum likelihood
because it is simpler.

Determine the following estimates.
\begin{gather*}
  P(F=1|C=0), P(F=1|C=1), \\ P(G=1|C=0), P(G=1|C=1), \\
  P(F=0,G=0|C=0), P(F=0,G=1|C=0), \\ P(F=1,G=0|C=0), P(F=1,G=1|C=0), \\
  P(F=0,G=0|C=1), P(F=0,G=1|C=1), \\ P(F=1,G=0|C=1), P(F=1,G=1|C=1).
\end{gather*}
\tjboxed{%
 \begin{alignat*}{3}
     P(F=1|C=0)&=\frac{6}{17}=0.3529   &\qquad     P(F=1|C=1)&=\frac{143}{196}=0.7296 \\
     P(G=1|C=0)&=\frac{28}{51}=0.5490  &\qquad     P(G=1|C=1)&=\frac{93}{196}=0.4745 \\ 
 P(F=0,G=0|C=0)&=\frac{5}{17}=0.2941   &\qquad P(F=0,G=1|C=0)&=\frac{6}{17}=0.3529 \\
 P(F=1,G=0|C=0)&=\frac{8}{51}=0.1569   &\qquad P(F=1,G=1|C=0)&=\frac{10}{51}=0.1961 \\
 P(F=0,G=0|C=1)&=\frac{1}{7}=0.1429    &\qquad P(F=0,G=1|C=1)&=\frac{25}{196}=0.1276 \\
 P(F=1,G=0|C=1)&=\frac{75}{196}=0.3827 &\qquad P(F=1,G=1|C=1)&=\frac{17}{49}=0.3469 
 \end{alignat*}
}
\end{deelvraag}
%
Model $M_1$ for e-mail does not consider any feature. So $P(C)$ can be used to estimate the probability that the next e-mail will be spam or not. We will write that as $P(C|M_1)$.\\
\begin{deelvraag}
  Model $M_2$ considers only feature $F$ to predict whether the next e-mail will be spam or not.
  
  Use Bayes rule and the probability estimates determined in the previous question
  to determine an estimate for $P(C|M_2)=P(C|F)$.
  \tjboxed{%
    \begin{alignat*}{2}
    \text{Bayes} && \qquad P(C=1|F) &= \frac{P(C=1)P(F|C=1)}{P(F)}, \\
    \text{where} && \qquad P(F) &= P(C=0)P(F|C=0)+P(C=1)P(F|C=1). \\
    \text{We get} && \qquad P(C=1|F=0) &= \frac{53}{86} = 0.6163 \\
    \text{and} && \qquad P(C=1|F=1) &= \frac{143}{161} = 0.8882.
    \end{alignat*}
  }
\end{deelvraag}
Model $M_3$ considers feature $G$ only and model $M_4$ considers both $F$ and $G$. Model $M_5$ also considers both $F$ and $G$ but assumes that $F$ and $G$ are independent given the classification $C$.
\begin{deelvraag}
  Use Bayes rule again to show how you would calculate $P(C|M_5)$.
  \tjboxed{%
    \begin{alignat*}{2}
    \text{Bayes} && \qquad P(C=1|M_5) &= \frac{P(C=1)P(F|C=1)P(G|C=1)}{P(F,G)}, \\
    \text{where} && \qquad P(F,G) &= P(C=0)P(F|C=0)P(G|C=0)+\\
                 && \qquad        &\quad + P(C=1)P(F|C=1)P(G|C=1). \\
    \text{We get} && \qquad P(C=1|F=0,G=0) &= \frac{92803}{142391} = 0.6517 \\
                  && \qquad P(C=1|F=0,G=1) &= \frac{83793}{144161} = 0.5812 \\
                  && \qquad P(C=1|F=1,G=0) &= \frac{250393}{277441} = 0.9025 \\
    \text{and}    && \qquad P(C=1|F=1,G=1) &= \frac{75361}{86337} = 0.8729.
    \end{alignat*}
  }
\end{deelvraag}
\begin{deelvraag}
  The models $M_1, M_2,\ldots,M_5$ all have a certain number of free parameters.
  Determine the number of free parameters for each of the five models.
  \tjboxed{%
    Model 1: No features so we consider the joint probability $P\{C\}$ only.
Thus we have one free parameter, for instance $P\{C=1\}$.
\newline
Answer: 1 free parameter
\\[\baselineskip]
Model 2: The joint probability is $P\{C,F\} = P\{C\}P\{F|C\}$.
The free parameters are $P\{C=1\}$, 
$P\{F=1|C=0\}$, $P\{F=1|C=1\}$.
\newline
Answer: 3 free parameters
\\[\baselineskip]
Model 3: The joint probability is $P\{C,G\} = P\{C\}P\{G|C\}$.
The free parameters are $P\{C=1\}$, 
$P\{G=1|C=0\}$, $P\{G=1|C=1\}$.
\newline
Answer: 3 free parameters
\\[\baselineskip]
Model 4: The joint probability is $P\{C,F,G\} = P\{C\}P\{F|C\}P\{G|C,F\}$.
The free parameters are $P\{C=1\}$, 
$P\{F=1|C=0\}$, $P\{F=1|C=1\}$,
$P\{G=1|C=0,F=0\}$, $P\{G=1|C=0,F=1\}$,
$P\{G=1|C=1,F=0\}$, $P\{G=1|C=1,F=1\}$.
\newline
Answer: 7 free parameters
\\[\baselineskip]
Model 5: The joint probability is $P\{C,F,G\} = P\{C\}P\{F|C\}P\{G|C\}$.
The free parameters are $P\{C=1\}$, 
$P\{F=1|C=0\}$, $P\{F=1|C=1\}$,
$P\{G=1|C=0\}$, $P\{G=1|C=1\}$.
\newline
Answer: 5 free parameters
  }
\end{deelvraag}
\begin{deelvraag}
  Given the training set of the 247 e-mail as shown in the table above,
  which of the five models would you prefer? Use an MDL argument in your answer.

  HINT: You will need to calculate an estimate for the email entropy for each model.
  For model $M_1$ you make an estimate of $H(C)$ using the maximum likelihood estimate $P(C=1)=0.7935$.
  Likewise you calculate for $M_2$ the entropy $H(C|F)$ and thus you'll need to compute $P(C,F)$.
  For $M_3$ you must compute the entropy $H(C|G)$; for $M_4$
  you calculate $H(C|F,G)$ and for $M_5$ also $H(C|F,G)$ although this will be a different calculation
  than for $M_4$.
  \tjboxed{%
    As Stochastic complexity we use the formula:
    \[ \text{sc} = \#\text{param}/2 \log N + N H(C|\text{relevant params}), \]
    Where N is the number of e-mails (247).
  }
  \tjboxed{%
    First we compute the five entropies using the general formula
    \begin{align*}
      H(C|X) &= \sum_{x\in\mathcal{X}}P_X(x)H(C|X=x) \\
      H(C|X=x) &= \sum_{c=0}^1 -P_{C|X}(c|x)\log_2 P_{C|X}(c|x) \\
      P_{C|X}(c|x) &= \frac{P_{C,X}(c,x)}{P_{X}(x)} \\
      P_{X}(x) &= \sum_{c'=0}^1 P_{C,X}(c',x)
    \end{align*}
    \begin{alignat*}{2}
      H(M_1) &= H(C) = 0.7347, && \quad \text{$X$ is empty, so $P(C,X)=P(C)$} \\
      H(M_2) &= H(C|F) = 0.6639, && \quad \text{$X=F$, so $P(C,X)=P(C)P(F|C)$} \\
      H(M_3) &= H(C|G) = 0.7321, && \quad \text{$X=G$, so $P(C,X)=P(C)P(G|C)$} \\
      H(M_4) &= H(C|F,G) = 0.6614, && \quad \text{$X=F,G$, so $P(C,X)=P(C)P(F|C)P(G|C,F)$} \\
      H(M_5) &= H(C|F,G) = 0.6615, && \quad \text{$X=F,G$, so $P(C,X)=P(C)P(F|C)P(G|C)$}
    \end{alignat*}
  }
  \tjboxed{%
    The stochastic complexities are:
    \begin{align*}
    \text{sc}(M_1) &= 1\times\frac12\log_2 N + NH(C) = 185.444 \\
    \text{sc}(M_2) &= 3\times\frac12\log_2 N + NH(C|F) = 175.894 \\
    \text{sc}(M_3) &= 3\times\frac12\log_2 N + NH(C|G) = 192.743 \\
    \text{sc}(M_4) &= 7\times\frac12\log_2 N + NH(C|F,G) = 191.175 \\
    \text{sc}(M_5) &= 5\times\frac12\log_2 N + NH(C|F,G) = 183.259 \\
    \end{align*}
    Based on this we should prefer $M_2$, which has the lowest stochastic complexity.
    \\[\baselineskip]
    Note that eventually, we would prefer $M4$ because for very long sequences the entropy
    is the most important term and $H(M_4)$ is the smallest of the five.
  }
\end{deelvraag}
\end{vraag}

\newpage
\section*{Appendix: formula sheet}

%Consider random variables (RV) $x\in\mathcal{X}$ and $y\in \mathcal{Y}$. The basic axioms of probability theory are the sum and product rules,
%$$
%p(x) + p(\overline{x}) = 1 \quad \text{(sum rule)}
%$$
%$$
%p(x,y) = p(x|y)p(y) \quad \text{(product rule)}
%$$
%
%The following formulas can be derived from the sum and product rules,
%$$
%p(x|y) = \frac{p(x,y)}{p(y)} \quad \text{(conditional probability of $x$, given $y$)}
%$$
%
%$$
%p(x|y) = \frac{p(y|x)p(x)}{p(y)} = \frac{p(y|x)p(x)}{\sum_{x\in\mathcal{X}}p(y|x)p(x)} \quad \text{(Bayes rule)}
%$$
%
%$$
%p(x) = \sum_{y\in\mathcal{Y}} p(x,y) \quad \text{(marginal probability)}
%$$
%
%\medskip
%We use for mean and variance the following notation,
%
%$$
%\varepsilon[x] = \langle x \rangle = \int_x x p(x) \d{x} \quad \text{(expectation)}
%$$
%$$
%\var[x] = \varepsilon[(x-\varepsilon[x])(x-\varepsilon[x])^T] \quad \text{(variance)}
%$$
%\medskip
%\textbf{Some probability distributions}\\
The \emph{Bernoulli distribution} is a discrete distribution having two possible outcomes labeled by $x = 0$ and $x = 1$ in which $x = 1$ ("success") occurs with probability $\theta$ and $x = 0$ ("failure") occurs with probability $1-\theta$. It therefore has probability function
\begin{equation}
p(x|\theta) =\theta^x(1-\theta)^{1-x}
\tag{A.1}
\end{equation}

The \emph{Gaussian distribution} with mean $\mu$ and variance $\sigma^2$ is defined as
$$
\N(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}
$$
\end{exam}
\end{document}
